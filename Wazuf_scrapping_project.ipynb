{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from itertools import zip_longest\n",
        "def extract_job_data(page_num):\n",
        "    try:\n",
        "        result = requests.get(f\"https://wuzzuf.net/search/jobs/?a=spbg&q=python&start={page_num}\")\n",
        "        src = result.content\n",
        "        soup = BeautifulSoup(src, \"lxml\")\n",
        "\n",
        "        job_titles = soup.find_all(\"h2\", {\"class\": \"css-m604qf\"})\n",
        "        job_descriptions = soup.find_all(\"div\", {\"class\": \"css-y4udm8\"})\n",
        "        company_names = soup.find_all(\"a\", {\"class\": \"css-17s97q8\"})\n",
        "        location_names = soup.find_all(\"span\", {\"class\": \"css-5wys0k\"})\n",
        "\n",
        "        job_data = []\n",
        "        for title, desc, company, location in zip_longest(job_titles, job_descriptions, company_names, location_names):\n",
        "            data = {\n",
        "                \"title\": title.text.strip() if title else None,\n",
        "                \"description\": desc.text.strip() if desc else None,\n",
        "                \"company\": company.text.strip() if company else None,\n",
        "                \"location\": location.text.strip() if location else None\n",
        "            }\n",
        "            job_data.append(data)\n",
        "\n",
        "        return job_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", e)\n",
        "        return []\n",
        "\n",
        "def save_to_file(job_data, file_name):\n",
        "    try:\n",
        "        with open(file_name, 'w', encoding='utf-8') as file:\n",
        "            for job in job_data:\n",
        "                file.write(f\"Title: {job['title']}\\n\")\n",
        "                file.write(f\"Company: {job['company']}\\n\")\n",
        "                file.write(f\"Description: {job['description']}\\n\")\n",
        "                file.write(f\"Location: {job['location']}\\n\\n\")\n",
        "        print(f\"Data saved to {file_name}\")\n",
        "    except Exception as e:\n",
        "        print(\"Error occurred while saving to file:\", e)\n",
        "\n",
        "def crawl_jobs():\n",
        "    try:\n",
        "        page_num = 0\n",
        "        while True:\n",
        "            job_data = extract_job_data(page_num)\n",
        "            if not job_data:\n",
        "                print(\"No more jobs to crawl.\")\n",
        "                break\n",
        "\n",
        "            file_name = f\"job_data_{page_num}.txt\"\n",
        "            save_to_file(job_data, file_name)\n",
        "\n",
        "            page_num += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", e)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    crawl_jobs()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Sb57pwXMbIS",
        "outputId": "00f4bfcc-bd1a-4aae-dece-e9182a606011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to job_data_0.txt\n",
            "Data saved to job_data_1.txt\n",
            "Data saved to job_data_2.txt\n",
            "Data saved to job_data_3.txt\n",
            "Data saved to job_data_4.txt\n",
            "Data saved to job_data_5.txt\n",
            "Data saved to job_data_6.txt\n",
            "Data saved to job_data_7.txt\n",
            "Data saved to job_data_8.txt\n",
            "Data saved to job_data_9.txt\n",
            "Data saved to job_data_10.txt\n",
            "Data saved to job_data_11.txt\n",
            "Data saved to job_data_12.txt\n",
            "Data saved to job_data_13.txt\n",
            "Data saved to job_data_14.txt\n",
            "Data saved to job_data_15.txt\n",
            "Data saved to job_data_16.txt\n",
            "No more jobs to crawl.\n"
          ]
        }
      ]
    }
  ]
}